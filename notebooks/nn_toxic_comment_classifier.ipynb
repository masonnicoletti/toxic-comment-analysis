{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2411e806",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a95e2373-5cf1-43ff-934f-55b2e1a28eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/sfs/gpfs/tardis/home/dpv8cf/toxic-comment-analysis'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/sfs/gpfs/tardis/home/dpv8cf/toxic-comment-analysis\")  # adjust if needed\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c9c8032-507c-43b7-a61a-d20ba41eafb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "sys.path.append(\"scripts\")\n",
    "import data_cleaning\n",
    "from torch.utils.data import TensorDataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dd6e014-3353-4684-a918-6c25c86b2d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73eb12bc-49d4-4b5e-a63b-2e8baba67263",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_cleaning.clean_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4294ee64-f855-4227-9354-fd419e60bef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1593229, 15),\n",
       " Index(['comment_text', 'funny', 'wow', 'sad', 'likes', 'disagree', 'toxicity',\n",
       "        'severe_toxicity', 'obscene', 'sexual_explicit', 'identity_attack',\n",
       "        'insult', 'threat', 'toxicity_annotator_count', 'rating_rejected'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape, data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cffd4b-efdd-4568-8cbc-d6cb7c8833d3",
   "metadata": {},
   "source": [
    "## Comment Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec7354a4-d89a-48cd-afdd-2905c62f887a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "\n",
    "# Using full dataset to create a smaller stratified sample\n",
    "X_comment_only = data[\"comment_text\"].astype(str)\n",
    "y = data[\"rating_rejected\"]\n",
    "\n",
    "sss = StratifiedShuffleSplit(\n",
    "    n_splits=1,\n",
    "    train_size=100_000,        # try 50_000 if still too slow\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Get indices for the small, stratified sample\n",
    "idx_small, _ = next(sss.split(X_comment_only, y))\n",
    "\n",
    "# Create a smaller DataFrame to run NN + param search on\n",
    "data_small = data.iloc[idx_small].copy()\n",
    "\n",
    "#Smaller Sample\n",
    "X_comment_only = data_small[\"comment_text\"].astype(str)\n",
    "y = data_small[\"rating_rejected\"]\n",
    "\n",
    "#train/val split on the SMALL sample\n",
    "X_train_comment_only, X_val_comment_only, y_train, y_val = train_test_split(\n",
    "    X_comment_only,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "param_grid = []\n",
    "\n",
    "activations = [\"relu\", \"tanh\"]\n",
    "hidden_sizes = [128, 256]\n",
    "lrs = [0.001, 0.01]\n",
    "\n",
    "for act in activations:\n",
    "    for h in hidden_sizes:\n",
    "        for lr in lrs:\n",
    "            param_grid.append({\n",
    "                \"activation\": act,\n",
    "                \"hidden_size\": h,\n",
    "                \"lr\": lr\n",
    "            })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bf66ee9-845a-4896-ac7f-a052ebd206d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 20002\n",
      "======================================================================\n",
      "Testing config: {'activation': 'relu', 'hidden_size': 128, 'lr': 0.001}\n",
      "======================================================================\n",
      "Epoch 1 | Train Loss: 0.2529 | Val F1: 0.0000\n",
      "Epoch 2 | Train Loss: 0.2196 | Val F1: 0.0105\n",
      "Epoch 3 | Train Loss: 0.1934 | Val F1: 0.0531\n",
      "Epoch 4 | Train Loss: 0.1726 | Val F1: 0.1032\n",
      "Epoch 5 | Train Loss: 0.1571 | Val F1: 0.1262\n",
      "Epoch 6 | Train Loss: 0.1458 | Val F1: 0.1174\n",
      "Epoch 7 | Train Loss: 0.1361 | Val F1: 0.1362\n",
      "Epoch 8 | Train Loss: 0.1270 | Val F1: 0.1210\n",
      "Epoch 9 | Train Loss: 0.1203 | Val F1: 0.1303\n",
      "Epoch 10 | Train Loss: 0.1135 | Val F1: 0.1312\n",
      "Early stopping.\n",
      "Best Val F1 for config {'activation': 'relu', 'hidden_size': 128, 'lr': 0.001}: 0.1362\n",
      "\n",
      "======================================================================\n",
      "Testing config: {'activation': 'relu', 'hidden_size': 128, 'lr': 0.01}\n",
      "======================================================================\n",
      "Epoch 1 | Train Loss: 0.2386 | Val F1: 0.0076\n",
      "Epoch 2 | Train Loss: 0.1949 | Val F1: 0.0939\n",
      "Epoch 3 | Train Loss: 0.1553 | Val F1: 0.0754\n",
      "Epoch 4 | Train Loss: 0.1144 | Val F1: 0.1050\n",
      "Epoch 5 | Train Loss: 0.0853 | Val F1: 0.1237\n",
      "Epoch 6 | Train Loss: 0.0717 | Val F1: 0.1394\n",
      "Epoch 7 | Train Loss: 0.0566 | Val F1: 0.1323\n",
      "Epoch 8 | Train Loss: 0.0464 | Val F1: 0.1422\n",
      "Epoch 9 | Train Loss: 0.0396 | Val F1: 0.1356\n",
      "Epoch 10 | Train Loss: 0.0418 | Val F1: 0.1437\n",
      "Epoch 11 | Train Loss: 0.0352 | Val F1: 0.1397\n",
      "Epoch 12 | Train Loss: 0.0318 | Val F1: 0.1462\n",
      "Epoch 13 | Train Loss: 0.0284 | Val F1: 0.1452\n",
      "Epoch 14 | Train Loss: 0.0310 | Val F1: 0.1572\n",
      "Epoch 15 | Train Loss: 0.0293 | Val F1: 0.1365\n",
      "Epoch 16 | Train Loss: 0.0280 | Val F1: 0.1393\n",
      "Epoch 17 | Train Loss: 0.0270 | Val F1: 0.1585\n",
      "Epoch 18 | Train Loss: 0.0245 | Val F1: 0.1632\n",
      "Epoch 19 | Train Loss: 0.0253 | Val F1: 0.1567\n",
      "Epoch 20 | Train Loss: 0.0213 | Val F1: 0.1491\n",
      "Best Val F1 for config {'activation': 'relu', 'hidden_size': 128, 'lr': 0.01}: 0.1632\n",
      "\n",
      "======================================================================\n",
      "Testing config: {'activation': 'relu', 'hidden_size': 256, 'lr': 0.001}\n",
      "======================================================================\n",
      "Epoch 1 | Train Loss: 0.2521 | Val F1: 0.0000\n",
      "Epoch 2 | Train Loss: 0.2189 | Val F1: 0.0076\n",
      "Epoch 3 | Train Loss: 0.1925 | Val F1: 0.0345\n",
      "Epoch 4 | Train Loss: 0.1719 | Val F1: 0.0733\n",
      "Epoch 5 | Train Loss: 0.1563 | Val F1: 0.1017\n",
      "Epoch 6 | Train Loss: 0.1443 | Val F1: 0.1183\n",
      "Epoch 7 | Train Loss: 0.1339 | Val F1: 0.1284\n",
      "Epoch 8 | Train Loss: 0.1251 | Val F1: 0.1076\n",
      "Epoch 9 | Train Loss: 0.1173 | Val F1: 0.1222\n",
      "Epoch 10 | Train Loss: 0.1097 | Val F1: 0.1250\n",
      "Early stopping.\n",
      "Best Val F1 for config {'activation': 'relu', 'hidden_size': 256, 'lr': 0.001}: 0.1284\n",
      "\n",
      "======================================================================\n",
      "Testing config: {'activation': 'relu', 'hidden_size': 256, 'lr': 0.01}\n",
      "======================================================================\n",
      "Epoch 1 | Train Loss: 0.2391 | Val F1: 0.0180\n",
      "Epoch 2 | Train Loss: 0.1954 | Val F1: 0.0658\n",
      "Epoch 3 | Train Loss: 0.1531 | Val F1: 0.0933\n",
      "Epoch 4 | Train Loss: 0.1126 | Val F1: 0.1144\n",
      "Epoch 5 | Train Loss: 0.0804 | Val F1: 0.1144\n",
      "Epoch 6 | Train Loss: 0.0666 | Val F1: 0.1350\n",
      "Epoch 7 | Train Loss: 0.0544 | Val F1: 0.1243\n",
      "Epoch 8 | Train Loss: 0.0441 | Val F1: 0.1345\n",
      "Epoch 9 | Train Loss: 0.0367 | Val F1: 0.1200\n",
      "Early stopping.\n",
      "Best Val F1 for config {'activation': 'relu', 'hidden_size': 256, 'lr': 0.01}: 0.1350\n",
      "\n",
      "======================================================================\n",
      "Testing config: {'activation': 'tanh', 'hidden_size': 128, 'lr': 0.001}\n",
      "======================================================================\n",
      "Epoch 1 | Train Loss: 0.2509 | Val F1: 0.0015\n",
      "Epoch 2 | Train Loss: 0.2209 | Val F1: 0.0220\n",
      "Epoch 3 | Train Loss: 0.1951 | Val F1: 0.0527\n",
      "Epoch 4 | Train Loss: 0.1745 | Val F1: 0.0702\n",
      "Epoch 5 | Train Loss: 0.1587 | Val F1: 0.0782\n",
      "Epoch 6 | Train Loss: 0.1474 | Val F1: 0.1084\n",
      "Epoch 7 | Train Loss: 0.1376 | Val F1: 0.1102\n",
      "Epoch 8 | Train Loss: 0.1299 | Val F1: 0.1300\n",
      "Epoch 9 | Train Loss: 0.1228 | Val F1: 0.1190\n",
      "Epoch 10 | Train Loss: 0.1173 | Val F1: 0.1404\n",
      "Epoch 11 | Train Loss: 0.1118 | Val F1: 0.1320\n",
      "Epoch 12 | Train Loss: 0.1076 | Val F1: 0.1291\n",
      "Epoch 13 | Train Loss: 0.1030 | Val F1: 0.1446\n",
      "Epoch 14 | Train Loss: 0.0999 | Val F1: 0.1349\n",
      "Epoch 15 | Train Loss: 0.0970 | Val F1: 0.1351\n",
      "Epoch 16 | Train Loss: 0.0937 | Val F1: 0.1418\n",
      "Early stopping.\n",
      "Best Val F1 for config {'activation': 'tanh', 'hidden_size': 128, 'lr': 0.001}: 0.1446\n",
      "\n",
      "======================================================================\n",
      "Testing config: {'activation': 'tanh', 'hidden_size': 128, 'lr': 0.01}\n",
      "======================================================================\n",
      "Epoch 1 | Train Loss: 0.2380 | Val F1: 0.0221\n",
      "Epoch 2 | Train Loss: 0.2000 | Val F1: 0.0659\n",
      "Epoch 3 | Train Loss: 0.1711 | Val F1: 0.1277\n",
      "Epoch 4 | Train Loss: 0.1464 | Val F1: 0.1139\n",
      "Epoch 5 | Train Loss: 0.1269 | Val F1: 0.1516\n",
      "Epoch 6 | Train Loss: 0.1092 | Val F1: 0.1403\n",
      "Epoch 7 | Train Loss: 0.0959 | Val F1: 0.1409\n",
      "Epoch 8 | Train Loss: 0.0866 | Val F1: 0.1252\n",
      "Early stopping.\n",
      "Best Val F1 for config {'activation': 'tanh', 'hidden_size': 128, 'lr': 0.01}: 0.1516\n",
      "\n",
      "======================================================================\n",
      "Testing config: {'activation': 'tanh', 'hidden_size': 256, 'lr': 0.001}\n",
      "======================================================================\n",
      "Epoch 1 | Train Loss: 0.2451 | Val F1: 0.0030\n",
      "Epoch 2 | Train Loss: 0.2188 | Val F1: 0.0207\n",
      "Epoch 3 | Train Loss: 0.1935 | Val F1: 0.0541\n",
      "Epoch 4 | Train Loss: 0.1734 | Val F1: 0.0803\n",
      "Epoch 5 | Train Loss: 0.1581 | Val F1: 0.0995\n",
      "Epoch 6 | Train Loss: 0.1467 | Val F1: 0.1020\n",
      "Epoch 7 | Train Loss: 0.1369 | Val F1: 0.1363\n",
      "Epoch 8 | Train Loss: 0.1291 | Val F1: 0.1426\n",
      "Epoch 9 | Train Loss: 0.1225 | Val F1: 0.1319\n",
      "Epoch 10 | Train Loss: 0.1166 | Val F1: 0.1360\n",
      "Epoch 11 | Train Loss: 0.1120 | Val F1: 0.1238\n",
      "Early stopping.\n",
      "Best Val F1 for config {'activation': 'tanh', 'hidden_size': 256, 'lr': 0.001}: 0.1426\n",
      "\n",
      "======================================================================\n",
      "Testing config: {'activation': 'tanh', 'hidden_size': 256, 'lr': 0.01}\n",
      "======================================================================\n",
      "Epoch 1 | Train Loss: 0.2387 | Val F1: 0.0030\n",
      "Epoch 2 | Train Loss: 0.2043 | Val F1: 0.0331\n",
      "Epoch 3 | Train Loss: 0.1770 | Val F1: 0.1141\n",
      "Epoch 4 | Train Loss: 0.1548 | Val F1: 0.1173\n",
      "Epoch 5 | Train Loss: 0.1341 | Val F1: 0.1383\n",
      "Epoch 6 | Train Loss: 0.1172 | Val F1: 0.1411\n",
      "Epoch 7 | Train Loss: 0.1035 | Val F1: 0.1303\n",
      "Epoch 8 | Train Loss: 0.0919 | Val F1: 0.1520\n",
      "Epoch 9 | Train Loss: 0.0827 | Val F1: 0.1593\n",
      "Epoch 10 | Train Loss: 0.0729 | Val F1: 0.1669\n",
      "Epoch 11 | Train Loss: 0.0663 | Val F1: 0.1482\n",
      "Epoch 12 | Train Loss: 0.0598 | Val F1: 0.1594\n",
      "Epoch 13 | Train Loss: 0.0536 | Val F1: 0.1468\n",
      "Early stopping.\n",
      "Best Val F1 for config {'activation': 'tanh', 'hidden_size': 256, 'lr': 0.01}: 0.1669\n",
      "\n",
      "\n",
      "Best Config: {'activation': 'tanh', 'hidden_size': 256, 'lr': 0.01}\n",
      "Best Validation F1: 0.16686069072565\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# ======================================================\n",
    "# 1. Tokenizer\n",
    "# ======================================================\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    return text.lower().strip().split() #split on space\n",
    "\n",
    "#Simple tokenizer with lowercase + split\n",
    "\n",
    "# ======================================================\n",
    "# 2. Build vocabulary from TRAIN ONLY\n",
    "# ======================================================\n",
    "from collections import Counter\n",
    "#Counting all words appearing in training comments\n",
    "word_counter = Counter()\n",
    "\n",
    "for text in X_train_comment_only:\n",
    "    tokens = simple_tokenize(text) #Convert Strings into Tokens\n",
    "    word_counter.update(tokens) #Count word frequencies\n",
    "\n",
    "max_vocab_size = 20000 #Keep only top 20k words\n",
    "most_common = word_counter.most_common(max_vocab_size)\n",
    "#Special Token for Padding and Unknown Words\n",
    "PAD = \"<PAD>\"\n",
    "UNK = \"<UNK>\"\n",
    "#Creating Vocabulary Lists\n",
    "idx2word = [PAD, UNK] + [w for w, _ in most_common] #Index -> Words\n",
    "word2idx = {w: i for i, w in enumerate(idx2word)} #Words -> Index\n",
    "\n",
    "pad_idx = word2idx[PAD]\n",
    "unk_idx = word2idx[UNK]\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "# ======================================================\n",
    "# 3. Convert text → sequence\n",
    "# ======================================================\n",
    "\n",
    "def text_to_seq(text, max_len=100):\n",
    "    tokens = simple_tokenize(text) #Tokenize\n",
    "    seq = [word2idx.get(tok, unk_idx) for tok in tokens] #Map to Indices\n",
    "    if len(seq) < max_len:\n",
    "        seq = seq + [pad_idx] * (max_len - len(seq)) #Pad at End\n",
    "    else:\n",
    "        seq = seq[:max_len] #Cut off Extra Words\n",
    "    return np.array(seq, dtype=np.int64)\n",
    "#Convert Train and Validation Into Integer Sequences\n",
    "max_len = 100\n",
    "\n",
    "X_train_seq = np.stack([text_to_seq(t) for t in X_train_comment_only])\n",
    "X_val_seq   = np.stack([text_to_seq(t) for t in X_val_comment_only])\n",
    "\n",
    "# ======================================================\n",
    "# 4. Dataset + DataLoader\n",
    "# ======================================================\n",
    "\n",
    "class CommentDataset(Dataset):\n",
    "    def __init__(self, X_seq, y):\n",
    "        self.X = torch.from_numpy(X_seq).long() #Convert to Tensor\n",
    "        self.y = torch.from_numpy(y.values).long()  # pandas Series → tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = CommentDataset(X_train_seq, y_train)\n",
    "val_dataset   = CommentDataset(X_val_seq,   y_val)\n",
    "#DataLoaders for Batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ======================================================\n",
    "# 5. Neural Network\n",
    "# ======================================================\n",
    "\n",
    "def get_activation(name):\n",
    "    if name == \"relu\":\n",
    "        return nn.ReLU()\n",
    "    elif name == \"tanh\":\n",
    "        return nn.Tanh()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid activation\")\n",
    "#Neural Network Model\n",
    "class CommentNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, activation, pad_idx):\n",
    "        super().__init__() #Embedding Layer: Converts Word Indices -> Dense Vectors\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embed_dim,\n",
    "            padding_idx=pad_idx #Pad Tokens stay zero\n",
    "        )\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_size),\n",
    "            get_activation(activation), #ReLU or TanH\n",
    "            nn.Dropout(0.3), #Regularization\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            nn.Sigmoid() #Binary Classification \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len)\n",
    "        emb = self.embedding(x)        # (batch, seq_len, embed_dim)\n",
    "        pooled = emb.mean(dim=1)       # (batch, embed_dim)\n",
    "        return self.net(pooled)\n",
    "\n",
    "# ======================================================\n",
    "# 6. Early Stopping Training Loop\n",
    "# ======================================================\n",
    "\n",
    "def train_model(model, optimizer, train_loader, val_loader,\n",
    "                max_epochs=20, patience=3, device=\"cpu\"):\n",
    "\n",
    "    criterion = nn.BCELoss() #Binary Cross Entropy\n",
    "    model.to(device)\n",
    "\n",
    "    best_f1 = 0 #Track Best Validation F1\n",
    "    best_state = None #Store best model weights\n",
    "    wait = 0 #Patience Counter\n",
    "\n",
    "    for epoch in range(1, max_epochs+1):\n",
    "        # ---- TRAIN ----\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device).float().unsqueeze(1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        # ---- VALIDATION ----\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_targets = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "\n",
    "                probs = model(xb)\n",
    "                preds = (probs > 0.5).long().cpu().numpy().flatten()\n",
    "\n",
    "                val_preds.extend(preds.tolist())\n",
    "                val_targets.extend(yb.cpu().numpy().tolist())\n",
    "\n",
    "        val_f1 = f1_score(val_targets, val_preds)\n",
    "        print(f\"Epoch {epoch} | Train Loss: {np.mean(train_losses):.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        # ---- EARLY STOPPING ----\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_state = model.state_dict()\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "    # load best weights\n",
    "    model.load_state_dict(best_state)\n",
    "    return best_f1, model\n",
    "\n",
    "# ======================================================\n",
    "# 7. Hyperparameter Search (your param_grid)\n",
    "# ======================================================\n",
    "\n",
    "results = [] #Store Results \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "embed_dim = 100  # you can tune this later\n",
    "\n",
    "for params in param_grid:\n",
    "    print(\"=\"*70)\n",
    "    print(\"Testing config:\", params)\n",
    "    print(\"=\"*70)\n",
    "#Build Model with chosen hyperparameters\n",
    "    model = CommentNN(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=embed_dim,\n",
    "        hidden_size=params[\"hidden_size\"],\n",
    "        activation=params[\"activation\"],\n",
    "        pad_idx=pad_idx\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "#Train and Evaluate\n",
    "    best_f1, trained_model = train_model(\n",
    "        model,\n",
    "        optimizer,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        max_epochs=20,\n",
    "        patience=3,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    print(f\"Best Val F1 for config {params}: {best_f1:.4f}\\n\")\n",
    "    results.append((params, best_f1, trained_model))\n",
    "\n",
    "# ======================================================\n",
    "# 8. Best Hyperparameters\n",
    "# ======================================================\n",
    "\n",
    "best_params, best_val_f1, best_model = max(results, key=lambda x: x[1])\n",
    "print(\"\\nBest Config:\", best_params)\n",
    "print(\"Best Validation F1:\", best_val_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f11408-93b8-431e-b839-f626ee0c2fe0",
   "metadata": {},
   "source": [
    "To build a neural network that can understand raw text comments, the code first creates a smaller, balanced subset of the dataset for efficient training. Each comment is tokenized, meaning the text is converted into lowercase words split by spaces, and a vocabulary is built from the training data that assigns every word an integer ID. Because neural networks require fixed-size inputs, each comment is then transformed into a sequence of exactly 100 word indices: shorter comments are padded by adding special <PAD> tokens at the end, while longer comments are trimmed by cutting off words beyond the maximum length. These fixed-length sequences are fed into an embedding layer, which learns a dense vector representation for each word so that semantically similar words end up with similar numerical vectors. The embeddings for all words in a comment are then averaged to produce a single feature vector summarizing the entire comment. This vector is passed through a small feed-forward neural network with ReLU or Tanh activation and a final sigmoid output to predict whether the comment is toxic or rejected. The model is trained with early stopping and evaluated using validation F1 scores while searching across different hyperparameter combinations to find the best-performing architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d386cc39-75d3-4c37-8063-098d0b55f800",
   "metadata": {},
   "source": [
    "### Why Embeddings Instead of TF-IDF?\n",
    "\n",
    "In the comment-only neural network, I used word embeddings instead of TF-IDF because embeddings allow the model to learn its own numeric representation of each word during training. TF-IDF manually assigns each word a fixed numerical weight based on frequency, but it does not capture meaning, context, or relationships between words. Embeddings, on the other hand, transform each word into a dense vector that the model continuously adjusts so that words with similar meanings end up with similar representations. In simple terms, both TF-IDF and embeddings convert text into numbers so a model can use it, but embeddings are learned automatically, while TF-IDF is hand-crafted and fixed. For raw text and neural networks, embeddings are more appropriate because they allow the model to learn richer patterns beyond simple word counts, making them a more flexible and expressive encoding method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5a606c-edcc-4a73-8679-797f816681aa",
   "metadata": {},
   "source": [
    "## Numeric Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea4c4909-7562-4d22-bada-0b21a121f463",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [\n",
    "    \"funny\", \"wow\", \"sad\", \"likes\", \"disagree\",\n",
    "    \"toxicity\", \"severe_toxicity\", \"obscene\", \"sexual_explicit\",\n",
    "    \"identity_attack\", \"insult\", \"threat\", \"toxicity_annotator_count\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "758de56b-fdc1-4821-8dc7-69f44377a1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num = data[numeric_cols].values.astype(np.float32)\n",
    "y_num = data[\"rating_rejected\"].values  # 0 = not toxic, 1 = toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c97be3e4-77b1-4f51-b041-366994b51e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num, X_val_num, y_train_num, y_val_num = train_test_split(\n",
    "    X_num,\n",
    "    y_num,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y_num\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91fccc5f-1281-496e-942f-d29512118d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_num_scaled = scaler.fit_transform(X_train_num).astype(np.float32)\n",
    "X_val_num_scaled   = scaler.transform(X_val_num).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "171a9464-ad21-4c44-a7d0-e36a58a5a445",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "num_model = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),      # normalize numeric features\n",
    "    (\"clf\", LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        class_weight=\"balanced\"        # helps with imbalance\n",
    "    ))\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7561c0d3-bf2b-4ea8-af2c-2eebef239f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=42, shuffle=True),\n",
       "             estimator=Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                                       ('clf',\n",
       "                                        LogisticRegression(class_weight='balanced',\n",
       "                                                           max_iter=1000))]),\n",
       "             n_jobs=-1, param_grid={'clf__C': [0.1, 1.0, 10.0]}, scoring='f1',\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "param_grid_num = {\n",
    "    \"clf__C\": [0.1, 1.0, 10.0]    # low → more regularization, high → less\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid_num = GridSearchCV(\n",
    "    estimator=num_model,\n",
    "    param_grid=param_grid_num,\n",
    "    cv=cv,\n",
    "    scoring=\"f1\",\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_num.fit(X_train_num, y_train_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1be27e6a-b905-4c12-b0d2-ab55e66e4654",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumericDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.y = torch.from_numpy(y).long()  # 0/1\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "batch_size = 64\n",
    "train_num_dataset = NumericDataset(X_train_num_scaled, y_train_num)\n",
    "val_num_dataset   = NumericDataset(X_val_num_scaled,   y_val_num)\n",
    "\n",
    "train_num_loader = DataLoader(train_num_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_num_loader   = DataLoader(val_num_dataset,   batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f75dd818-7857-482c-a562-b7c3b6845aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(name):\n",
    "    name = name.lower()\n",
    "    if name == \"relu\":\n",
    "        return nn.ReLU()\n",
    "    elif name == \"tanh\":\n",
    "        return nn.Tanh()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown activation: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04af0174-8529-435c-9b6d-bbcea7bc4dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, activation):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_size),\n",
    "            get_activation(activation),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            nn.Sigmoid()   # outputs P(toxic=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44edc39f-6907-4593-a9a4-bd97223c0789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_num_model(model, optimizer, train_loader, val_loader,\n",
    "                    max_epochs=20, patience=3, device=\"cpu\"):\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "    model.to(device)\n",
    "\n",
    "    best_f1 = 0.0\n",
    "    best_state = None\n",
    "    wait = 0\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        # ---- TRAIN ----\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device).float().unsqueeze(1)  # (batch, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            probs = model(xb)                        # (batch, 1), values in [0,1]\n",
    "            loss = criterion(probs, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        avg_train_loss = float(np.mean(train_losses))\n",
    "\n",
    "        # ---- VALIDATION ----\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_targets = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "\n",
    "                probs = model(xb)                    # (batch, 1)\n",
    "                preds = (probs > 0.5).long().cpu().numpy().flatten()  # 0/1\n",
    "\n",
    "                val_preds.extend(preds.tolist())\n",
    "                val_targets.extend(yb.cpu().numpy().tolist())\n",
    "\n",
    "        val_f1 = f1_score(val_targets, val_preds)\n",
    "        print(f\"Epoch {epoch} | Train Loss: {avg_train_loss:.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        # ---- EARLY STOPPING ----\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_state = model.state_dict()\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    return best_f1, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5778ab1c-fb45-4d00-9dd5-756a16fb3803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Numbers-only NN config: {'activation': 'relu', 'hidden_size': 128, 'lr': 0.001}\n",
      "======================================================================\n",
      "Epoch 1 | Train Loss: 0.1998 | Val F1: 0.1952\n",
      "Epoch 2 | Train Loss: 0.1949 | Val F1: 0.2087\n",
      "Epoch 3 | Train Loss: 0.1944 | Val F1: 0.1449\n",
      "Epoch 4 | Train Loss: 0.1941 | Val F1: 0.1640\n",
      "Epoch 5 | Train Loss: 0.1940 | Val F1: 0.1594\n",
      "Early stopping.\n",
      "Best Val F1 for config {'activation': 'relu', 'hidden_size': 128, 'lr': 0.001}: 0.2087\n",
      "\n",
      "======================================================================\n",
      "Numbers-only NN config: {'activation': 'relu', 'hidden_size': 128, 'lr': 0.01}\n",
      "======================================================================\n",
      "Epoch 1 | Train Loss: 0.2036 | Val F1: 0.2292\n",
      "Epoch 2 | Train Loss: 0.2018 | Val F1: 0.1035\n",
      "Epoch 3 | Train Loss: 0.2011 | Val F1: 0.1842\n",
      "Epoch 4 | Train Loss: 0.2010 | Val F1: 0.1451\n",
      "Early stopping.\n",
      "Best Val F1 for config {'activation': 'relu', 'hidden_size': 128, 'lr': 0.01}: 0.2292\n",
      "\n",
      "======================================================================\n",
      "Numbers-only NN config: {'activation': 'relu', 'hidden_size': 256, 'lr': 0.001}\n",
      "======================================================================\n",
      "Epoch 1 | Train Loss: 0.1992 | Val F1: 0.1827\n",
      "Epoch 2 | Train Loss: 0.1942 | Val F1: 0.1694\n",
      "Epoch 3 | Train Loss: 0.1937 | Val F1: 0.2171\n",
      "Epoch 4 | Train Loss: 0.1935 | Val F1: 0.1485\n",
      "Epoch 5 | Train Loss: 0.1933 | Val F1: 0.1708\n",
      "Epoch 6 | Train Loss: 0.1933 | Val F1: 0.1993\n",
      "Early stopping.\n",
      "Best Val F1 for config {'activation': 'relu', 'hidden_size': 256, 'lr': 0.001}: 0.2171\n",
      "\n",
      "======================================================================\n",
      "Numbers-only NN config: {'activation': 'relu', 'hidden_size': 256, 'lr': 0.01}\n",
      "======================================================================\n",
      "Epoch 1 | Train Loss: 0.2047 | Val F1: 0.0836\n",
      "Epoch 2 | Train Loss: 0.2022 | Val F1: 0.0000\n",
      "Epoch 3 | Train Loss: 0.2018 | Val F1: 0.2290\n",
      "Epoch 4 | Train Loss: 0.2013 | Val F1: 0.0283\n",
      "Epoch 5 | Train Loss: 0.2013 | Val F1: 0.0471\n",
      "Epoch 6 | Train Loss: 0.2009 | Val F1: 0.1592\n",
      "Early stopping.\n",
      "Best Val F1 for config {'activation': 'relu', 'hidden_size': 256, 'lr': 0.01}: 0.2290\n",
      "\n",
      "======================================================================\n",
      "Numbers-only NN config: {'activation': 'tanh', 'hidden_size': 128, 'lr': 0.001}\n",
      "======================================================================\n",
      "Epoch 1 | Train Loss: 0.1998 | Val F1: 0.2063\n",
      "Epoch 2 | Train Loss: 0.1943 | Val F1: 0.1833\n",
      "Epoch 3 | Train Loss: 0.1932 | Val F1: 0.1705\n",
      "Epoch 4 | Train Loss: 0.1926 | Val F1: 0.1893\n",
      "Early stopping.\n",
      "Best Val F1 for config {'activation': 'tanh', 'hidden_size': 128, 'lr': 0.001}: 0.2063\n",
      "\n",
      "======================================================================\n",
      "Numbers-only NN config: {'activation': 'tanh', 'hidden_size': 128, 'lr': 0.01}\n",
      "======================================================================\n",
      "Epoch 1 | Train Loss: 0.2025 | Val F1: 0.1121\n",
      "Epoch 2 | Train Loss: 0.2009 | Val F1: 0.1896\n",
      "Epoch 3 | Train Loss: 0.2010 | Val F1: 0.2860\n",
      "Epoch 4 | Train Loss: 0.2008 | Val F1: 0.1258\n",
      "Epoch 5 | Train Loss: 0.2010 | Val F1: 0.2216\n",
      "Epoch 6 | Train Loss: 0.2008 | Val F1: 0.2216\n",
      "Early stopping.\n",
      "Best Val F1 for config {'activation': 'tanh', 'hidden_size': 128, 'lr': 0.01}: 0.2860\n",
      "\n",
      "======================================================================\n",
      "Numbers-only NN config: {'activation': 'tanh', 'hidden_size': 256, 'lr': 0.001}\n",
      "======================================================================\n",
      "Epoch 1 | Train Loss: 0.1994 | Val F1: 0.0832\n",
      "Epoch 2 | Train Loss: 0.1944 | Val F1: 0.2038\n",
      "Epoch 3 | Train Loss: 0.1932 | Val F1: 0.2072\n",
      "Epoch 4 | Train Loss: 0.1927 | Val F1: 0.1764\n",
      "Epoch 5 | Train Loss: 0.1925 | Val F1: 0.1900\n",
      "Epoch 6 | Train Loss: 0.1922 | Val F1: 0.2169\n",
      "Epoch 7 | Train Loss: 0.1919 | Val F1: 0.2065\n",
      "Epoch 8 | Train Loss: 0.1919 | Val F1: 0.2072\n",
      "Epoch 9 | Train Loss: 0.1918 | Val F1: 0.2105\n",
      "Early stopping.\n",
      "Best Val F1 for config {'activation': 'tanh', 'hidden_size': 256, 'lr': 0.001}: 0.2169\n",
      "\n",
      "======================================================================\n",
      "Numbers-only NN config: {'activation': 'tanh', 'hidden_size': 256, 'lr': 0.01}\n",
      "======================================================================\n",
      "Epoch 1 | Train Loss: 0.2088 | Val F1: 0.2868\n",
      "Epoch 2 | Train Loss: 0.2081 | Val F1: 0.2083\n",
      "Epoch 3 | Train Loss: 0.2087 | Val F1: 0.0770\n",
      "Epoch 4 | Train Loss: 0.2086 | Val F1: 0.0366\n",
      "Early stopping.\n",
      "Best Val F1 for config {'activation': 'tanh', 'hidden_size': 256, 'lr': 0.01}: 0.2868\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_grid_numNN = []\n",
    "\n",
    "activations = [\"relu\", \"tanh\"]\n",
    "hidden_sizes = [128, 256]\n",
    "lrs = [0.001, 0.01]\n",
    "\n",
    "for act in activations:\n",
    "    for h in hidden_sizes:\n",
    "        for lr in lrs:\n",
    "            param_grid_numNN.append({\n",
    "                \"activation\": act,\n",
    "                \"hidden_size\": h,\n",
    "                \"lr\": lr\n",
    "            })\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "input_dim = X_train_num_scaled.shape[1]\n",
    "\n",
    "results_numNN = []\n",
    "\n",
    "for params in param_grid_numNN:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Numbers-only NN config:\", params)\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    model = NumNN(\n",
    "        input_dim=input_dim,\n",
    "        hidden_size=params[\"hidden_size\"],\n",
    "        activation=params[\"activation\"]\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "\n",
    "    best_f1, trained_model = train_num_model(\n",
    "        model,\n",
    "        optimizer,\n",
    "        train_num_loader,\n",
    "        val_num_loader,\n",
    "        max_epochs=20,\n",
    "        patience=3,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    print(f\"Best Val F1 for config {params}: {best_f1:.4f}\\n\")\n",
    "    results_numNN.append((params, best_f1, trained_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "67ba8cb4-9136-4f1c-9c1a-635fa1b0db97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best numbers-only NN config: {'activation': 'tanh', 'hidden_size': 256, 'lr': 0.01}\n",
      "Best validation F1 (numbers-only NN): 0.28675617437427486\n"
     ]
    }
   ],
   "source": [
    "best_params_numNN, best_val_f1_numNN, best_model_numNN = max(\n",
    "    results_numNN,\n",
    "    key=lambda x: x[1]\n",
    ")\n",
    "\n",
    "print(\"Best numbers-only NN config:\", best_params_numNN)\n",
    "print(\"Best validation F1 (numbers-only NN):\", best_val_f1_numNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb70950-c50a-44e5-86f9-97468ced042c",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "The comment-only neural network achieved a validation F1 score of 0.17, which indicates that raw text alone is not highly predictive in this dataset. Many comments are extremely short or ambiguous, and the simple embedding + mean-pooling architecture does not capture deeper context or subtle forms of toxicity. In contrast, the numbers-only neural network—which uses reaction counts and toxicity-related metadata such as toxicity, obscene, insult, and identity_attack—performed substantially better with an F1 score of 0.29. For the numbers-only model, the best-performing hyperparameters were the Tanh activation function, a 256-unit hidden layer, and a learning rate of 0.01. These numeric features contain clearer and more direct signals associated with toxic behavior, which explains their stronger predictive power. This comparison shows that numeric metadata is more informative than raw text alone and motivates the use of combined models that integrate both text and numeric signals."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (toxic venv)",
   "language": "python",
   "name": "toxic-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
