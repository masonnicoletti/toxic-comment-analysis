{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1bee7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9115d53",
   "metadata": {},
   "source": [
    "DIRECTIONS: do NOT run the clean script from ./scripts before this. Instead, run all cells in ./clean_svc.ipynb before running the cells below this text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28c2606",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/train_clean.csv')\n",
    "# df = pd.read_csv('../data/train.csv') # when running just the clean script instead of ./clean_scv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84e6a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.000000\n",
      "1    0.000000\n",
      "2    0.714286\n",
      "3    0.000000\n",
      "4    0.000000\n",
      "Name: toxicity, dtype: float64\n",
      "\n",
      "\n",
      "Index(['comment_text', 'funny', 'wow', 'sad', 'likes', 'disagree', 'toxicity',\n",
      "       'severe_toxicity', 'obscene', 'sexual_explicit', 'identity_attack',\n",
      "       'insult', 'threat', 'toxicity_annotator_count', 'rating_rejected'],\n",
      "      dtype='object')\n",
      "\n",
      "\n",
      "0.0 1.0\n",
      "0 That was the reason Walker fire everyone now what ?.\n",
      "1 So my original statement still stands . A church needs to not get involved in supporting Candidate A as opposed to Candidate B, it must however must make a stand on the side of the Gospel and help its congregation be informed and allow it to vote on that basis...\n",
      "2 All you ever do is come to these forums berating white people with your Critical Race Theory garbage.\n",
      "\n",
      "Please study Islamic Law before stupidly comparing someone you speculate is fueled by a verboten emotion to ISIS. \n",
      "\n",
      "Good grief, study something more than CRT.\n",
      "3 Tonight will be an excellent opportunity for Clayton Kershaw to show that his bad postseason reputation is exaggerated, a bad rap. In his two postseason starts this year, his ERA is 4.76. He has also given up 5 HRs. After a superb August, Kershaw's ERA in September was 3.48, not terrible, but not up to his overall season performance. Simply put, Kershaw has not been overpowering for a while, and he's been susceptible to the long ball. But he can change all that tonight.\n",
      "\n",
      "Also of note, Clayton Kershaw has an opt out clause in his contract at the end of 2018. Probably not many Dodger fans are thinking about that today, but tonight's game may resonate down the road as that approaches, one way or the other.\n",
      "4 I read about this on Stars and Stripes oversea and I couldn't understand why the lake became an ocean. But after visiting Skilak Lake for the first time about 12 years after the event, I really could understand why such a tragic event took place.\n"
     ]
    }
   ],
   "source": [
    "# EDA\n",
    "\n",
    "print(df['toxicity'].head())\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(df.columns)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(np.min(df['toxicity']), np.max(df['toxicity']))\n",
    "\n",
    "\n",
    "for idx, text in df[\"comment_text\"].head(5).items():\n",
    "    print(idx, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cd5d91",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "584e0880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making label of toxicity feature into binary from continuous (0.0 to 1.0)\n",
    "\n",
    "df[\"comment_text\"] = df[\"comment_text\"].fillna(\"\").astype(str)\n",
    "df[\"label\"] = (df[\"toxicity\"] >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "09efc0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in cleaned df: ['comment_text', 'funny', 'wow', 'sad', 'likes', 'disagree', 'toxicity', 'severe_toxicity', 'obscene', 'sexual_explicit', 'identity_attack', 'insult', 'threat', 'toxicity_annotator_count', 'rating_rejected', 'label']\n",
      "Numeric feature columns (first few): ['funny', 'wow', 'sad', 'likes', 'disagree', 'severe_toxicity', 'obscene', 'sexual_explicit', 'identity_attack', 'insult']\n"
     ]
    }
   ],
   "source": [
    "# chosen columns\n",
    "# cols = ['rating', 'funny', 'wow', 'sad', 'likes',\n",
    "#        'disagree', 'sexual_explicit',\n",
    "#        'male', 'female', 'transgender',\n",
    "#        'other_gender', 'heterosexual', 'homosexual_gay_or_lesbian', 'bisexual',\n",
    "#        'other_sexual_orientation', 'christian', 'jewish', 'muslim', 'hindu',\n",
    "#        'buddhist', 'atheist', 'other_religion', 'black', 'white', 'asian',\n",
    "#        'latino', 'other_race_or_ethnicity', 'physical_disability',\n",
    "#        'intellectual_or_learning_disability', 'psychiatric_or_mental_illness',\n",
    "#        'other_disability']\n",
    "\n",
    "text_col = \"comment_text\"\n",
    "y = df[\"label\"]\n",
    "\n",
    "cols = [\n",
    "    c for c in df.columns\n",
    "    if c not in [text_col, \"label\", \"toxicity\"]\n",
    "]\n",
    "\n",
    "# df[cols] = df[cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "# df[cols] = df[cols].fillna(0)\n",
    "\n",
    "print(\"Columns in cleaned df:\", df.columns.tolist())\n",
    "print(\"Numeric feature columns (first few):\", cols[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1f569e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, val_idx = train_test_split(\n",
    "    df.index,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df[\"label\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22e95ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df[\"label\"].loc[train_idx]\n",
    "y_val = df[\"label\"].loc[val_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1ca844",
   "metadata": {},
   "source": [
    "### Just text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6de85c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split (already done in data_prep.py and data_cleaning.py in ./scripts)\n",
    "X_train_text = df.loc[train_idx, \"comment_text\"]\n",
    "X_val_text   = df.loc[val_idx, \"comment_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "39adf29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9464170270456871\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97    293147\n",
      "           1       0.75      0.49      0.59     25499\n",
      "\n",
      "    accuracy                           0.95    318646\n",
      "   macro avg       0.86      0.74      0.78    318646\n",
      "weighted avg       0.94      0.95      0.94    318646\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# just LinearSVC with text in X_train or X_test\n",
    "clf_text = Pipeline([\n",
    "    (\"tfidf\", TfidfVectorizer( # strings to numeric\n",
    "        stop_words=\"english\",\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=5,\n",
    "        max_df=0.9\n",
    "    )),\n",
    "    (\"svm\", LinearSVC(random_state=42)) # regular linearSVC algorithm\n",
    "])\n",
    "\n",
    "clf_text.fit(X_train_text, y_train)\n",
    "y_pred_text = clf_text.predict(X_val_text)\n",
    "\n",
    "print(accuracy_score(y_val, y_pred_text))\n",
    "print(classification_report(y_val, y_pred_text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee326f46",
   "metadata": {},
   "source": [
    "### Other columns and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a3e3f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_both = df.loc[train_idx, [\"comment_text\"] + cols]\n",
    "X_val_both   = df.loc[val_idx, [\"comment_text\"] + cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c25c7f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kaylakim/anaconda3/lib/python3.11/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9824256384828305\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99    293147\n",
      "           1       0.90      0.87      0.89     25499\n",
      "\n",
      "    accuracy                           0.98    318646\n",
      "   macro avg       0.95      0.93      0.94    318646\n",
      "weighted avg       0.98      0.98      0.98    318646\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LinearSVC with text and other columns\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"text\", TfidfVectorizer(\n",
    "            stop_words=\"english\",\n",
    "            ngram_range=(1, 2)\n",
    "        ), \"comment_text\"),\n",
    "        (\"num\", StandardScaler(), cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "clf_both = Pipeline([\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"svm\", LinearSVC(random_state=42))\n",
    "])\n",
    "\n",
    "clf_both.fit(X_train_both, y_train)\n",
    "y_pred_both = clf_both.predict(X_val_both)\n",
    "\n",
    "print(accuracy_score(y_val, y_pred_both))\n",
    "print(classification_report(y_val, y_pred_both))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d110c3d0",
   "metadata": {},
   "source": [
    "### Just other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a066f85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_num = df.loc[train_idx, cols]\n",
    "X_val_num   = df.loc[val_idx, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c03b33f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98381275773115\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99    293147\n",
      "           1       0.90      0.90      0.90     25499\n",
      "\n",
      "    accuracy                           0.98    318646\n",
      "   macro avg       0.95      0.94      0.94    318646\n",
      "weighted avg       0.98      0.98      0.98    318646\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LinearSVC withOUT text and just other columns\n",
    "clf_num = Pipeline([\n",
    "    (\"ss\", StandardScaler()),\n",
    "    (\"svm\", LinearSVC(random_state=42))\n",
    "])\n",
    "\n",
    "clf_num.fit(X_train_num, y_train)\n",
    "y_pred_num = clf_num.predict(X_val_num)\n",
    "\n",
    "print(accuracy_score(y_val, y_pred_num))\n",
    "print(classification_report(y_val, y_pred_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817917d0",
   "metadata": {},
   "source": [
    "## Optimizing just Both Comment and Columns SVC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3be151db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proprocess and pipeline build\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"text\", TfidfVectorizer(\n",
    "            stop_words=\"english\"\n",
    "        ), \"comment_text\"),\n",
    "        (\"num\", StandardScaler(), cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "base_svm_both = Pipeline([\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"svm\", LinearSVC(random_state=42, max_iter=10000))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113e5d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter grid for combined model\n",
    "param_grid_both = {\n",
    "    \"preprocess__text__ngram_range\": [(1, 1), (1, 2)],\n",
    "    \"preprocess__text__min_df\": [2, 5, 10],\n",
    "    \"svm__C\": [0.1, 1, 5]\n",
    "}\n",
    "\n",
    "# actual optimization through GridSearchCV\n",
    "grid_both = GridSearchCV(\n",
    "    estimator=base_svm_both,\n",
    "    param_grid=param_grid_both,\n",
    "    cv=3, \n",
    "    scoring=\"f1\",\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# X_train_both and y_train from  unified split\n",
    "grid_both.fit(X_train_both, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0391d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params (combined SVM): {'preprocess__text__min_df': 10, 'preprocess__text__ngram_range': (1, 1), 'svm__C': 1}\n",
      "Best CV F1 (combined SVM): 0.5827025890088314\n",
      "Validation accuracy (combined, tuned): 0.9443350463474377\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97     38311\n",
      "           1       0.74      0.47      0.57      3331\n",
      "\n",
      "    accuracy                           0.94     41642\n",
      "   macro avg       0.85      0.73      0.77     41642\n",
      "weighted avg       0.94      0.94      0.94     41642\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the parameters and scores for best parameters\n",
    "print(\"Best params (combined SVM):\", grid_both.best_params_)\n",
    "print(\"Best CV F1 (combined SVM):\", grid_both.best_score_)\n",
    "\n",
    "best_svm_both = grid_both.best_estimator_\n",
    "\n",
    "# Evaluate on the validation set\n",
    "y_val_pred_both = best_svm_both.predict(X_val_both)\n",
    "\n",
    "print(\"Validation accuracy (combined, tuned):\", accuracy_score(y_val, y_val_pred_both))\n",
    "print(classification_report(y_val, y_val_pred_both))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
