{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a95e2373-5cf1-43ff-934f-55b2e1a28eb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/sfs/gpfs/tardis/home/dpv8cf/toxic-comment-analysis'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/sfs/gpfs/tardis/home/dpv8cf/toxic-comment-analysis\")  # adjust if needed\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c9c8032-507c-43b7-a61a-d20ba41eafb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "sys.path.append(\"scripts\")\n",
    "import data_cleaning\n",
    "from torch.utils.data import TensorDataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dd6e014-3353-4684-a918-6c25c86b2d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73eb12bc-49d4-4b5e-a63b-2e8baba67263",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_cleaning.clean_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4294ee64-f855-4227-9354-fd419e60bef7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1593229, 15),\n",
       " Index(['comment_text', 'funny', 'wow', 'sad', 'likes', 'disagree', 'toxicity',\n",
       "        'severe_toxicity', 'obscene', 'sexual_explicit', 'identity_attack',\n",
       "        'insult', 'threat', 'toxicity_annotator_count', 'rating_rejected'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape, data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cffd4b-efdd-4568-8cbc-d6cb7c8833d3",
   "metadata": {},
   "source": [
    "## Comment Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec7354a4-d89a-48cd-afdd-2905c62f887a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "\n",
    "# Using full dataset to create a smaller stratified sample\n",
    "X_comment_only = data[\"comment_text\"].astype(str)\n",
    "y = data[\"rating_rejected\"]\n",
    "\n",
    "sss = StratifiedShuffleSplit(\n",
    "    n_splits=1,\n",
    "    train_size=100_000,        # try 50_000 if still too slow\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Get indices for the small, stratified sample\n",
    "idx_small, _ = next(sss.split(X_comment_only, y))\n",
    "\n",
    "# Create a smaller DataFrame to run NN + param search on\n",
    "data_small = data.iloc[idx_small].copy()\n",
    "\n",
    "#Smaller Sample\n",
    "X_comment_only = data_small[\"comment_text\"].astype(str)\n",
    "y = data_small[\"rating_rejected\"]\n",
    "\n",
    "#train/val split on the SMALL sample\n",
    "X_train_comment_only, X_val_comment_only, y_train, y_val = train_test_split(\n",
    "    X_comment_only,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "param_grid = []\n",
    "\n",
    "activations = [\"relu\", \"tanh\"]\n",
    "hidden_sizes = [128, 256]\n",
    "lrs = [0.001, 0.01]\n",
    "\n",
    "for act in activations:\n",
    "    for h in hidden_sizes:\n",
    "        for lr in lrs:\n",
    "            param_grid.append({\n",
    "                \"activation\": act,\n",
    "                \"hidden_size\": h,\n",
    "                \"lr\": lr\n",
    "            })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bf66ee9-845a-4896-ac7f-a052ebd206d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 20002\n",
      "======================================================================\n",
      "Testing config: {'activation': 'relu', 'hidden_size': 128, 'lr': 0.001}\n",
      "======================================================================\n",
      "Epoch 1 | Train Loss: 0.2529 | Val F1: 0.0000\n",
      "Epoch 2 | Train Loss: 0.2196 | Val F1: 0.0105\n",
      "Epoch 3 | Train Loss: 0.1934 | Val F1: 0.0531\n",
      "Epoch 4 | Train Loss: 0.1726 | Val F1: 0.1032\n",
      "Epoch 5 | Train Loss: 0.1571 | Val F1: 0.1262\n",
      "Epoch 6 | Train Loss: 0.1458 | Val F1: 0.1174\n",
      "Epoch 7 | Train Loss: 0.1361 | Val F1: 0.1362\n",
      "Epoch 8 | Train Loss: 0.1270 | Val F1: 0.1210\n",
      "Epoch 9 | Train Loss: 0.1203 | Val F1: 0.1303\n",
      "Epoch 10 | Train Loss: 0.1135 | Val F1: 0.1312\n",
      "Early stopping.\n",
      "Best Val F1 for config {'activation': 'relu', 'hidden_size': 128, 'lr': 0.001}: 0.1362\n",
      "\n",
      "======================================================================\n",
      "Testing config: {'activation': 'relu', 'hidden_size': 128, 'lr': 0.01}\n",
      "======================================================================\n",
      "Epoch 1 | Train Loss: 0.2386 | Val F1: 0.0076\n",
      "Epoch 2 | Train Loss: 0.1949 | Val F1: 0.0939\n",
      "Epoch 3 | Train Loss: 0.1553 | Val F1: 0.0754\n",
      "Epoch 4 | Train Loss: 0.1144 | Val F1: 0.1050\n",
      "Epoch 5 | Train Loss: 0.0853 | Val F1: 0.1237\n",
      "Epoch 6 | Train Loss: 0.0717 | Val F1: 0.1394\n",
      "Epoch 7 | Train Loss: 0.0566 | Val F1: 0.1323\n",
      "Epoch 8 | Train Loss: 0.0464 | Val F1: 0.1422\n",
      "Epoch 9 | Train Loss: 0.0396 | Val F1: 0.1356\n",
      "Epoch 10 | Train Loss: 0.0418 | Val F1: 0.1437\n",
      "Epoch 11 | Train Loss: 0.0352 | Val F1: 0.1397\n",
      "Epoch 12 | Train Loss: 0.0318 | Val F1: 0.1462\n",
      "Epoch 13 | Train Loss: 0.0284 | Val F1: 0.1452\n",
      "Epoch 14 | Train Loss: 0.0310 | Val F1: 0.1572\n",
      "Epoch 15 | Train Loss: 0.0293 | Val F1: 0.1365\n",
      "Epoch 16 | Train Loss: 0.0280 | Val F1: 0.1393\n",
      "Epoch 17 | Train Loss: 0.0270 | Val F1: 0.1585\n",
      "Epoch 18 | Train Loss: 0.0245 | Val F1: 0.1632\n",
      "Epoch 19 | Train Loss: 0.0253 | Val F1: 0.1567\n",
      "Epoch 20 | Train Loss: 0.0213 | Val F1: 0.1491\n",
      "Best Val F1 for config {'activation': 'relu', 'hidden_size': 128, 'lr': 0.01}: 0.1632\n",
      "\n",
      "======================================================================\n",
      "Testing config: {'activation': 'relu', 'hidden_size': 256, 'lr': 0.001}\n",
      "======================================================================\n",
      "Epoch 1 | Train Loss: 0.2521 | Val F1: 0.0000\n",
      "Epoch 2 | Train Loss: 0.2189 | Val F1: 0.0076\n",
      "Epoch 3 | Train Loss: 0.1925 | Val F1: 0.0345\n",
      "Epoch 4 | Train Loss: 0.1719 | Val F1: 0.0733\n",
      "Epoch 5 | Train Loss: 0.1563 | Val F1: 0.1017\n",
      "Epoch 6 | Train Loss: 0.1443 | Val F1: 0.1183\n",
      "Epoch 7 | Train Loss: 0.1339 | Val F1: 0.1284\n",
      "Epoch 8 | Train Loss: 0.1251 | Val F1: 0.1076\n",
      "Epoch 9 | Train Loss: 0.1173 | Val F1: 0.1222\n",
      "Epoch 10 | Train Loss: 0.1097 | Val F1: 0.1250\n",
      "Early stopping.\n",
      "Best Val F1 for config {'activation': 'relu', 'hidden_size': 256, 'lr': 0.001}: 0.1284\n",
      "\n",
      "======================================================================\n",
      "Testing config: {'activation': 'relu', 'hidden_size': 256, 'lr': 0.01}\n",
      "======================================================================\n",
      "Epoch 1 | Train Loss: 0.2391 | Val F1: 0.0180\n",
      "Epoch 2 | Train Loss: 0.1954 | Val F1: 0.0658\n",
      "Epoch 3 | Train Loss: 0.1531 | Val F1: 0.0933\n",
      "Epoch 4 | Train Loss: 0.1126 | Val F1: 0.1144\n",
      "Epoch 5 | Train Loss: 0.0804 | Val F1: 0.1144\n",
      "Epoch 6 | Train Loss: 0.0666 | Val F1: 0.1350\n",
      "Epoch 7 | Train Loss: 0.0544 | Val F1: 0.1243\n",
      "Epoch 8 | Train Loss: 0.0441 | Val F1: 0.1345\n",
      "Epoch 9 | Train Loss: 0.0367 | Val F1: 0.1200\n",
      "Early stopping.\n",
      "Best Val F1 for config {'activation': 'relu', 'hidden_size': 256, 'lr': 0.01}: 0.1350\n",
      "\n",
      "======================================================================\n",
      "Testing config: {'activation': 'tanh', 'hidden_size': 128, 'lr': 0.001}\n",
      "======================================================================\n",
      "Epoch 1 | Train Loss: 0.2509 | Val F1: 0.0015\n",
      "Epoch 2 | Train Loss: 0.2209 | Val F1: 0.0220\n",
      "Epoch 3 | Train Loss: 0.1951 | Val F1: 0.0527\n",
      "Epoch 4 | Train Loss: 0.1745 | Val F1: 0.0702\n",
      "Epoch 5 | Train Loss: 0.1587 | Val F1: 0.0782\n",
      "Epoch 6 | Train Loss: 0.1474 | Val F1: 0.1084\n",
      "Epoch 7 | Train Loss: 0.1376 | Val F1: 0.1102\n",
      "Epoch 8 | Train Loss: 0.1299 | Val F1: 0.1300\n",
      "Epoch 9 | Train Loss: 0.1228 | Val F1: 0.1190\n",
      "Epoch 10 | Train Loss: 0.1173 | Val F1: 0.1404\n",
      "Epoch 11 | Train Loss: 0.1118 | Val F1: 0.1320\n",
      "Epoch 12 | Train Loss: 0.1076 | Val F1: 0.1291\n",
      "Epoch 13 | Train Loss: 0.1030 | Val F1: 0.1446\n",
      "Epoch 14 | Train Loss: 0.0999 | Val F1: 0.1349\n",
      "Epoch 15 | Train Loss: 0.0970 | Val F1: 0.1351\n",
      "Epoch 16 | Train Loss: 0.0937 | Val F1: 0.1418\n",
      "Early stopping.\n",
      "Best Val F1 for config {'activation': 'tanh', 'hidden_size': 128, 'lr': 0.001}: 0.1446\n",
      "\n",
      "======================================================================\n",
      "Testing config: {'activation': 'tanh', 'hidden_size': 128, 'lr': 0.01}\n",
      "======================================================================\n",
      "Epoch 1 | Train Loss: 0.2380 | Val F1: 0.0221\n",
      "Epoch 2 | Train Loss: 0.2000 | Val F1: 0.0659\n",
      "Epoch 3 | Train Loss: 0.1711 | Val F1: 0.1277\n",
      "Epoch 4 | Train Loss: 0.1464 | Val F1: 0.1139\n",
      "Epoch 5 | Train Loss: 0.1269 | Val F1: 0.1516\n",
      "Epoch 6 | Train Loss: 0.1092 | Val F1: 0.1403\n",
      "Epoch 7 | Train Loss: 0.0959 | Val F1: 0.1409\n",
      "Epoch 8 | Train Loss: 0.0866 | Val F1: 0.1252\n",
      "Early stopping.\n",
      "Best Val F1 for config {'activation': 'tanh', 'hidden_size': 128, 'lr': 0.01}: 0.1516\n",
      "\n",
      "======================================================================\n",
      "Testing config: {'activation': 'tanh', 'hidden_size': 256, 'lr': 0.001}\n",
      "======================================================================\n",
      "Epoch 1 | Train Loss: 0.2451 | Val F1: 0.0030\n",
      "Epoch 2 | Train Loss: 0.2188 | Val F1: 0.0207\n",
      "Epoch 3 | Train Loss: 0.1935 | Val F1: 0.0541\n",
      "Epoch 4 | Train Loss: 0.1734 | Val F1: 0.0803\n",
      "Epoch 5 | Train Loss: 0.1581 | Val F1: 0.0995\n",
      "Epoch 6 | Train Loss: 0.1467 | Val F1: 0.1020\n",
      "Epoch 7 | Train Loss: 0.1369 | Val F1: 0.1363\n",
      "Epoch 8 | Train Loss: 0.1291 | Val F1: 0.1426\n",
      "Epoch 9 | Train Loss: 0.1225 | Val F1: 0.1319\n",
      "Epoch 10 | Train Loss: 0.1166 | Val F1: 0.1360\n",
      "Epoch 11 | Train Loss: 0.1120 | Val F1: 0.1238\n",
      "Early stopping.\n",
      "Best Val F1 for config {'activation': 'tanh', 'hidden_size': 256, 'lr': 0.001}: 0.1426\n",
      "\n",
      "======================================================================\n",
      "Testing config: {'activation': 'tanh', 'hidden_size': 256, 'lr': 0.01}\n",
      "======================================================================\n",
      "Epoch 1 | Train Loss: 0.2387 | Val F1: 0.0030\n",
      "Epoch 2 | Train Loss: 0.2043 | Val F1: 0.0331\n",
      "Epoch 3 | Train Loss: 0.1770 | Val F1: 0.1141\n",
      "Epoch 4 | Train Loss: 0.1548 | Val F1: 0.1173\n",
      "Epoch 5 | Train Loss: 0.1341 | Val F1: 0.1383\n",
      "Epoch 6 | Train Loss: 0.1172 | Val F1: 0.1411\n",
      "Epoch 7 | Train Loss: 0.1035 | Val F1: 0.1303\n",
      "Epoch 8 | Train Loss: 0.0919 | Val F1: 0.1520\n",
      "Epoch 9 | Train Loss: 0.0827 | Val F1: 0.1593\n",
      "Epoch 10 | Train Loss: 0.0729 | Val F1: 0.1669\n",
      "Epoch 11 | Train Loss: 0.0663 | Val F1: 0.1482\n",
      "Epoch 12 | Train Loss: 0.0598 | Val F1: 0.1594\n",
      "Epoch 13 | Train Loss: 0.0536 | Val F1: 0.1468\n",
      "Early stopping.\n",
      "Best Val F1 for config {'activation': 'tanh', 'hidden_size': 256, 'lr': 0.01}: 0.1669\n",
      "\n",
      "\n",
      "Best Config: {'activation': 'tanh', 'hidden_size': 256, 'lr': 0.01}\n",
      "Best Validation F1: 0.16686069072565\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# ======================================================\n",
    "# 1. Tokenizer\n",
    "# ======================================================\n",
    "\n",
    "def simple_tokenize(text):\n",
    "    return text.lower().strip().split() #split on space\n",
    "\n",
    "#Simple tokenizer with lowercase + split\n",
    "\n",
    "# ======================================================\n",
    "# 2. Build vocabulary from TRAIN ONLY\n",
    "# ======================================================\n",
    "from collections import Counter\n",
    "#Counting all words appearing in training comments\n",
    "word_counter = Counter()\n",
    "\n",
    "for text in X_train_comment_only:\n",
    "    tokens = simple_tokenize(text) #Convert Strings into Tokens\n",
    "    word_counter.update(tokens) #Count word frequencies\n",
    "\n",
    "max_vocab_size = 20000 #Keep only top 20k words\n",
    "most_common = word_counter.most_common(max_vocab_size)\n",
    "#Special Token for Padding and Unknown Words\n",
    "PAD = \"<PAD>\"\n",
    "UNK = \"<UNK>\"\n",
    "#Creating Vocabulary Lists\n",
    "idx2word = [PAD, UNK] + [w for w, _ in most_common] #Index -> Words\n",
    "word2idx = {w: i for i, w in enumerate(idx2word)} #Words -> Index\n",
    "\n",
    "pad_idx = word2idx[PAD]\n",
    "unk_idx = word2idx[UNK]\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "# ======================================================\n",
    "# 3. Convert text → sequence\n",
    "# ======================================================\n",
    "\n",
    "def text_to_seq(text, max_len=100):\n",
    "    tokens = simple_tokenize(text) #Tokenize\n",
    "    seq = [word2idx.get(tok, unk_idx) for tok in tokens] #Map to Indices\n",
    "    if len(seq) < max_len:\n",
    "        seq = seq + [pad_idx] * (max_len - len(seq)) #Pad at End\n",
    "    else:\n",
    "        seq = seq[:max_len] #Cut off Extra Words\n",
    "    return np.array(seq, dtype=np.int64)\n",
    "#Convert Train and Validation Into Integer Sequences\n",
    "max_len = 100\n",
    "\n",
    "X_train_seq = np.stack([text_to_seq(t) for t in X_train_comment_only])\n",
    "X_val_seq   = np.stack([text_to_seq(t) for t in X_val_comment_only])\n",
    "\n",
    "# ======================================================\n",
    "# 4. Dataset + DataLoader\n",
    "# ======================================================\n",
    "\n",
    "class CommentDataset(Dataset):\n",
    "    def __init__(self, X_seq, y):\n",
    "        self.X = torch.from_numpy(X_seq).long() #Convert to Tensor\n",
    "        self.y = torch.from_numpy(y.values).long()  # pandas Series → tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = CommentDataset(X_train_seq, y_train)\n",
    "val_dataset   = CommentDataset(X_val_seq,   y_val)\n",
    "#DataLoaders for Batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# ======================================================\n",
    "# 5. Neural Network\n",
    "# ======================================================\n",
    "\n",
    "def get_activation(name):\n",
    "    if name == \"relu\":\n",
    "        return nn.ReLU()\n",
    "    elif name == \"tanh\":\n",
    "        return nn.Tanh()\n",
    "    else:\n",
    "        raise ValueError(\"Invalid activation\")\n",
    "#Neural Network Model\n",
    "class CommentNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_size, activation, pad_idx):\n",
    "        super().__init__() #Embedding Layer: Converts Word Indices -> Dense Vectors\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=embed_dim,\n",
    "            padding_idx=pad_idx #Pad Tokens stay zero\n",
    "        )\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_size),\n",
    "            get_activation(activation), #ReLU or TanH\n",
    "            nn.Dropout(0.3), #Regularization\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            nn.Sigmoid() #Binary Classification \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len)\n",
    "        emb = self.embedding(x)        # (batch, seq_len, embed_dim)\n",
    "        pooled = emb.mean(dim=1)       # (batch, embed_dim)\n",
    "        return self.net(pooled)\n",
    "\n",
    "# ======================================================\n",
    "# 6. Early Stopping Training Loop\n",
    "# ======================================================\n",
    "\n",
    "def train_model(model, optimizer, train_loader, val_loader,\n",
    "                max_epochs=20, patience=3, device=\"cpu\"):\n",
    "\n",
    "    criterion = nn.BCELoss() #Binary Cross Entropy\n",
    "    model.to(device)\n",
    "\n",
    "    best_f1 = 0 #Track Best Validation F1\n",
    "    best_state = None #Store best model weights\n",
    "    wait = 0 #Patience Counter\n",
    "\n",
    "    for epoch in range(1, max_epochs+1):\n",
    "        # ---- TRAIN ----\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device).float().unsqueeze(1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        # ---- VALIDATION ----\n",
    "        model.eval()\n",
    "        val_preds = []\n",
    "        val_targets = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "\n",
    "                probs = model(xb)\n",
    "                preds = (probs > 0.5).long().cpu().numpy().flatten()\n",
    "\n",
    "                val_preds.extend(preds.tolist())\n",
    "                val_targets.extend(yb.cpu().numpy().tolist())\n",
    "\n",
    "        val_f1 = f1_score(val_targets, val_preds)\n",
    "        print(f\"Epoch {epoch} | Train Loss: {np.mean(train_losses):.4f} | Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        # ---- EARLY STOPPING ----\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_state = model.state_dict()\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(\"Early stopping.\")\n",
    "                break\n",
    "\n",
    "    # load best weights\n",
    "    model.load_state_dict(best_state)\n",
    "    return best_f1, model\n",
    "\n",
    "# ======================================================\n",
    "# 7. Hyperparameter Search (your param_grid)\n",
    "# ======================================================\n",
    "\n",
    "results = [] #Store Results \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "embed_dim = 100  # you can tune this later\n",
    "\n",
    "for params in param_grid:\n",
    "    print(\"=\"*70)\n",
    "    print(\"Testing config:\", params)\n",
    "    print(\"=\"*70)\n",
    "#Build Model with chosen hyperparameters\n",
    "    model = CommentNN(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=embed_dim,\n",
    "        hidden_size=params[\"hidden_size\"],\n",
    "        activation=params[\"activation\"],\n",
    "        pad_idx=pad_idx\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params[\"lr\"])\n",
    "#Train and Evaluate\n",
    "    best_f1, trained_model = train_model(\n",
    "        model,\n",
    "        optimizer,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        max_epochs=20,\n",
    "        patience=3,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    print(f\"Best Val F1 for config {params}: {best_f1:.4f}\\n\")\n",
    "    results.append((params, best_f1, trained_model))\n",
    "\n",
    "# ======================================================\n",
    "# 8. Best Hyperparameters\n",
    "# ======================================================\n",
    "\n",
    "best_params, best_val_f1, best_model = max(results, key=lambda x: x[1])\n",
    "print(\"\\nBest Config:\", best_params)\n",
    "print(\"Best Validation F1:\", best_val_f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f11408-93b8-431e-b839-f626ee0c2fe0",
   "metadata": {},
   "source": [
    "To build a neural network that can understand raw text comments, the code first creates a smaller, balanced subset of the dataset for efficient training. Each comment is tokenized, meaning the text is converted into lowercase words split by spaces, and a vocabulary is built from the training data that assigns every word an integer ID. Because neural networks require fixed-size inputs, each comment is then transformed into a sequence of exactly 100 word indices: shorter comments are padded by adding special <PAD> tokens at the end, while longer comments are trimmed by cutting off words beyond the maximum length. These fixed-length sequences are fed into an embedding layer, which learns a dense vector representation for each word so that semantically similar words end up with similar numerical vectors. The embeddings for all words in a comment are then averaged to produce a single feature vector summarizing the entire comment. This vector is passed through a small feed-forward neural network with ReLU or Tanh activation and a final sigmoid output to predict whether the comment is toxic or rejected. The model is trained with early stopping and evaluated using validation F1 scores while searching across different hyperparameter combinations to find the best-performing architecture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (toxic venv)",
   "language": "python",
   "name": "toxic-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
